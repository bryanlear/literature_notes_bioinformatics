@article{Meng2023,
  author={Meng, L and Attali, R and Talmy, T and Regev, Y and Mizrahi, N and Smirin-Yosef, P and Vossaert, L and Taborda, C and Santana, M and Machol, I and Xiao, R and Dai, H and Eng, C and Xia, F and Tzur, S},
  title={Evaluation of an automated genome interpretation model for rare disease routinely used in a clinical genetic laboratory},
  journal={Genet Med},
  year={2023},
  volume={25},
  number={6},
  pages={100830},
  doi={10.1016/j.gim.2023.100830},
  pmid={36939041}
}

@article{liu2025deep,
  title={Deep learning-based ranking method for subgroup and predictive biomarker identification in patients},
  author={Liu, Zhen and Gu, Yifan and Huang, Xiaoyang},
  journal={Communications Medicine},
  volume={5},
  pages={221},
  year={2025},
  publisher={Nature Portfolio},
  doi={10.1038/s43856-025-00946-z},
  url={https://doi.org/10.1038/s43856-025-00946-z}
}

@article{
doi:10.1126/science.adg7492,
author = {Jun Cheng  and Guido Novati  and Joshua Pan  and Clare Bycroft  and Akvilė Žemgulytė  and Taylor Applebaum  and Alexander Pritzel  and Lai Hong Wong  and Michal Zielinski  and Tobias Sargeant  and Rosalia G. Schneider  and Andrew W. Senior  and John Jumper  and Demis Hassabis  and Pushmeet Kohli  and Žiga Avsec },
title = {Accurate proteome-wide missense variant effect prediction with AlphaMissense},
journal = {Science},
volume = {381},
number = {6664},
pages = {eadg7492},
year = {2023},
doi = {10.1126/science.adg7492},
URL = {https://www.science.org/doi/abs/10.1126/science.adg7492},
eprint = {https://www.science.org/doi/pdf/10.1126/science.adg7492},
abstract = {The vast majority of missense variants observed in the human genome are of unknown clinical significance. We present AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity. By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data. The average pathogenicity score of genes is also predictive for their cell essentiality, capable of identifying short essential genes that existing statistical approaches are underpowered to detect. As a resource to the community, we provide a database of predictions for all possible human single amino acid substitutions and classify 89\% of missense variants as either likely benign or likely pathogenic. Single–amino acid changes in proteins sometimes have little effect but can often lead to problems in protein folding, activity, or stability. Only a small fraction of variants have been experimentally investigated, but there are vast amounts of biological sequence data that are suitable for use as training data for machine learning approaches. Cheng et al. developed AlphaMissense, a deep learning model that builds on the protein structure prediction tool AlphaFold2 (see the Perspective by Marsh and Teichmann). The model is trained on population frequency data and uses sequence and predicted structural context, all of which contribute to its performance. The authors evaluated the model against related methods using clinical databases not included in the training and demonstrated agreement with multiplexed assays of variant effect. Predictions for all single–amino acid substitutions in the human proteome are provided as a community resource. —Michael A. Funk AlphaFold fine-tuned on human and primate population variant frequency databases predicts variant pathogenicity.}}

@article{Jumper2021,
  author={Jumper, J. and Evans, R. and Pritzel, A. and et al.},
  title={Highly accurate protein structure prediction with AlphaFold},
  journal={Nature},
  year={2021},
  volume={596},
  number={7873}, % Note: The provided text had "Issue Date 26 August 2021", but didn't explicitly list the issue number. 596, 583-589 typically implies issue 7873 for Nature. If you have the exact issue number, you can replace this. I've used the common issue number for this page range in that volume.
  pages={583--589},
  doi={10.1038/s41586-021-03819-2}
}
@article{Ng2003SIFT,
  author={Ng, P. C. and Henikoff, S.},
  title={SIFT: predicting amino acid changes that affect protein function},
  journal={Nucleic Acids Research},
  year={2003},
  volume={31},
  number={13},
  pages={3812--3814},
  doi={10.1093/nar/gkg509}
}

@article{10.1093/bioinformatics/btab083,
    author = {Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V},
    title = {DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome},
    journal = {Bioinformatics},
    volume = {37},
    number = {15},
    pages = {2112-2120},
    year = {2021},
    month = {02},
    abstract = {Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.The source code, pretrained and finetuned model for DNABERT are available at GitHub (https://github.com/jerryji1993/DNABERT).Supplementary data are available at Bioinformatics online.},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btab083},
    url = {https://doi.org/10.1093/bioinformatics/btab083},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/37/15/2112/57195892/btab083.pdf},
}



